{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La Régression Logistique, un problème de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons implémenter un algorithme de classification supervisée. Contrairement à la régression linéaire qui consiste à prédire une valeur scalaire, la régression logistique a pour but d'estimer la probabilité d'une variable catégorielle. Une variable catégorielle correspond à un nombre entier compris entre $1$ et $K$ pour un problème à $K$ classes d'objets où la notion de proximité (1 est plus proche de 2 que de 3) est oubliée. Nous considererons ici un cas simple à deux classes. Puis nous mettrons en place un classificateur de chiffre manuscrit compris entre 0 et 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La régression logistique** cherche à estimer la probabilité $\\mathbb{P}(y=1|\\boldsymbol{x})$ où $y\\in\\{0,1\\}$. On obtient la probabilité inverse de la manière suivante : $\\mathbb{P}(y=0|\\boldsymbol{x})$=1-$\\mathbb{P}(y=1|\\boldsymbol{x})$. Pour cela, on suppose que le paramètre naturel $\\eta$ de notre loi est estimable à partir d'une combinaison linéaire des variables explicatives :\n",
    "\\begin{equation*}\n",
    "\\exists\\boldsymbol{\\beta}\\in\\mathbb{R}^d,\\ \\eta(\\boldsymbol{x}) = \\langle\\boldsymbol{\\beta}, \\boldsymbol{x}\\rangle\n",
    "\\end{equation*}\n",
    "\n",
    "La fonction de lien $\\sigma$ est la fonction qui permet du passer du paramètre naturel à notre probabilité. Dans le cas d'une loi de Bernoulli (loi d'une variable binaire), la fonction de lien est la sigmoid :\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\sigma:\\mathbb{R}&\\mapsto \\big[0, 1\\big]\\\\\n",
    "z&\\Rightarrow (1+\\text{exp}(-z))^{-1}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "La fonction sigmoid est illustrée par la figure suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# configuration generale de matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 8.0)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = (1+np.exp(-x))**(-1)\n",
    "plt.plot(x, y, label=\"fonction sigmoid\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, si le paramètre naturel $\\eta$ est négatif, la probabilité estimée sera inférieur à $0.5$ et notre échantillon appartiendra plus probablement à la classse $0$ ($y=0$). À l'inverse, si $\\eta$ est positif, on dira que notre échantillon appartient à la classe positive.\n",
    "\n",
    "il est ainsi possible d'obtenir notre probabilité de la manière suivante :\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{P}(y_{\\text{new}}=1|\\boldsymbol{x_{\\text{new}}}, \\boldsymbol{\\beta})=\\sigma(\\boldsymbol{\\beta}^T\\boldsymbol{x_{\\text{new}}} ).\n",
    "\\end{equation}\n",
    "\n",
    "De la même manière que pour la régression linéaire, on supposera que le vecteur $\\boldsymbol{x}$ possède une dimension $0$ avec la valeur $1$ faisant office de biais.\n",
    "\n",
    "---\n",
    "On dit que deux vecteurs $u$ et $v$ sont orthogonaux si leur produit scalaire est nul :\n",
    "\\begin{equation}\n",
    "\\langle u, v \\rangle = 0\n",
    "\\end{equation}\n",
    "\n",
    "La frontière de décision est l'ensemble de tous les points qu'il n'est pas possible de classer $1$ ou $0$. C'est l'ensemble des points tels que $\\mathbb{P}(y=1|\\boldsymbol{x}, \\boldsymbol{\\beta})=0.5$. Dit encore autrement, et en nous référant à la figure ci-dessus, il s'agit de l'ensemble des points tels que le paramètre naturel estimé $\\eta(\\boldsymbol{x})=\\boldsymbol{\\beta}^T\\boldsymbol{x}=0$. Dit encore autrement, il s'agit de l'ensemble des points orthogonaux au vecteur de paramètres $\\boldsymbol{\\beta}$. Le vecteur $\\boldsymbol{\\beta}$ est appelé vecteur normal à l'hyperplan séparateur.  \n",
    "\n",
    "---\n",
    "\n",
    "La régression logistique nous donne la probabilité $\\mathbb{P}(y_{\\text{new}}=1|\\boldsymbol{x_{\\text{new}}}, \\boldsymbol{\\beta})$. Il est trivial d'obtenir la classe à partir de ce score. On dira que l'échantillon appartient à la classe $1$ si la probabilité est supérieure à $0.5$ et à la classe $0$ dans le cas contraire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction d'un jeu de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons le modèle génératif suivant :\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\beta} \\sim \\mathcal{N}(0, 1)^3 \\in \\mathbb{R}^3\n",
    "\\end{equation}\n",
    "Nos échantillons sont simulés via une loi normale de moyenne centrée sur la frontière de décision. Notons $\\boldsymbol{\\beta^\\prime}=\\begin{bmatrix}\\beta_1\\\\ \\beta_2\\end{bmatrix}$. On fixera cette moyenne de la manière suivante :\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\mu}=\\boldsymbol{\\beta^\\prime}\\Bigg(-\\frac{\\beta_0}{\\lVert \\boldsymbol{\\beta^\\prime}\\rVert^2}\n",
    "\\Bigg).\n",
    "\\end{equation}\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"color:blue\">**Exercice :**</span> **Vérifier qu'on obtient bien :\n",
    "\\begin{equation}\n",
    "\\langle \\boldsymbol{\\beta^\\prime}, \\boldsymbol{\\mu}\\rangle + \\beta_0=0\n",
    "\\end{equation}\n",
    "Dit autrement, il s'agit de vérifier que $\\boldsymbol{\\mu}$ est bien sur la frontière.**\n",
    "\n",
    "<span style=\"color:green\">**Réponse :**</span>\n",
    "\n",
    "---\n",
    "\n",
    "La moyenne de notre loi normale étant maintenant fixée, nous pouvons simuler nos données :\n",
    "\\begin{equation}\n",
    "\\boldsymbol{x}\\sim\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{1})\\in\\mathbb{R}^2\n",
    "\\end{equation}\n",
    "\n",
    "La classe d'un échantillon est donnée par :\n",
    "\\begin{equation}\n",
    "y_i=\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  1\\text{ si }\\langle\\boldsymbol{\\beta^\\prime},\\boldsymbol{x_i}\\rangle +\\beta_0>0\\\\\n",
    "                  0\\text{ sinon.}\n",
    "                \\end{array}\n",
    "              \\right.\n",
    "\\end{equation}\n",
    "\n",
    "Notre problème est donc par construction totalement linéairement séparable dans le sens où la frontière de décision définie par le vecteur normal $\\boldsymbol{\\beta^\\prime}$ et par le biais $\\beta_0$ sépare totalement et sans erreur notre jeu de données.\n",
    "\n",
    "Le code ci dessous affiche le jeux de données ainsi que la représentation graphique de la frontière de decision $f(x)=-\\frac{\\beta_1}{\\beta_2}x_1-\\frac{\\beta_0}{\\beta_2}$. On vérifie facilement que le vecteur construit tel que $\\beta_2=f(x)$ pour $\\beta_1$ et $\\beta_0$ quelconques (à part les cas particuliers) sont bien sur la frontière."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "real_beta = np.random.normal(0, 1, size=3)\n",
    "\n",
    "def sample_data(n, beta):\n",
    "    # constructing mean \n",
    "    mu = beta[1:3]*(-beta[0]/(np.linalg.norm(beta[1:3])**2))\n",
    "    # covariance is the same for each class\n",
    "    cov  = np.diag(np.ones(2))\n",
    "    \n",
    "    # sampling x and adding the bias\n",
    "    X = np.insert(np.random.multivariate_normal(mu, cov, size=n), 0, 1, axis=1)\n",
    "    \n",
    "    # the label is deterministic\n",
    "    y = (np.dot(X, beta)>0)*1\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "X, y = sample_data(100, real_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 8.0)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot(X, y, beta=None, predictor=None, title=None):\n",
    "    ymin_ = X[:,2].min()\n",
    "    ymax_ = X[:,2].max()\n",
    "    min_ = X[:,1].min()\n",
    "    max_ = X[:,1].max()\n",
    "    \n",
    "    if predictor is not None:\n",
    "        h = 0.02\n",
    "        xx, yy = np.meshgrid(np.arange(min_, max_, h), np.arange(ymin_, ymax_, h))\n",
    "        Z = predictor.predict(np.insert(np.c_[xx.ravel(), yy.ravel()], 0, 1, axis=1))\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.pcolormesh(xx, yy, Z,shading='auto', alpha=0.01)\n",
    "    \n",
    "    plt.scatter(X[:,1], X[:,2], c=y)\n",
    "    \n",
    "    if beta is not None:\n",
    "        x_ = np.linspace(min_, max_, 500)\n",
    "        y_  = -beta[0]/beta[2] - x_ * beta[1] / beta[2]\n",
    "        plt.plot(x_, y_)\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.xlim(min_, max_)\n",
    "    plt.ylim(ymin_, ymax_)\n",
    "    plt.show()\n",
    "plot(X, y, real_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction objectif et gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la même manière que pour la régression nous pouvons obtenir notre fonction objectif à partir de la formulation de la vraisemblance de notre problème :\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{\\boldsymbol{\\beta}}(\\mathcal{S})=\\prod_{(\\boldsymbol{x}, y)\\in\\mathcal{S}}\\mathbb{P}(y=1|\\boldsymbol{x},\\boldsymbol{\\beta})^y\\mathbb{P}(y=0|\\boldsymbol{x},\\boldsymbol{\\beta})^{1-y}\n",
    "\\end{equation}\n",
    "\n",
    "Le paramètre maximisant la vraisemblance est aussi celui minimisant la log vraisemblance négative :\n",
    "\\begin{equation}\n",
    "-\\text{log}\\big(\\mathcal{L}_{\\boldsymbol{\\beta}}(\\mathcal{S})\\big)=-\\sum_{(\\boldsymbol{x}, y)\\in\\mathcal{S}}y\\text{log}(p)+(1-y)\\text{log}(1-p)\n",
    "\\end{equation}\n",
    "\n",
    "où $p=\\mathbb{P}(y=1|\\boldsymbol{x},\\boldsymbol{\\beta})=\\sigma(\\boldsymbol{\\beta}^T\\boldsymbol{x})$. Cette fonction objectif, ou *loss* s'appelle la *cross entropy* ou entropie croisée. On obtient donc :\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{\\beta}}=\\text{argmin}_{\\boldsymbol{\\beta}}\\Big[-\\sum_{(\\boldsymbol{x}, y)\\in\\mathcal{S}}y\\text{log}(\\sigma(\\boldsymbol{\\beta}^T\\boldsymbol{x}))+(1-y)\\text{log}(1-\\sigma(\\boldsymbol{\\beta}^T\\boldsymbol{x}))\\Big]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:blue\">**Question 1 :**</span> **Compléter la méthode $\\texttt{val}$ de l'objet $\\texttt{CrossEntropy}$ ci-dessous.**\n",
    "\n",
    "\n",
    "<span style=\"color:blue\">**Question 2 :**</span> **Calculez les dérivées partielles $\\frac{\\partial J(\\beta)}{\\partial \\beta_0}$ et $\\frac{\\partial J(\\beta)}{\\partial \\beta_1}$ de la fonction de coût de notre modèle de régréssion linéaire. Complétez la méthode $\\texttt{grad}$ de l'objet $\\texttt{CrossEntropy}$ ci dessous.**\n",
    "\n",
    "**<span style=\"color:orange\">Indice</span>**  Rappellez-vous que la dérivée d'une composition de fonction s'écrit $(g \\circ f)\\prime (x) = f\\prime(x) g\\prime(f(x))$ et que la fonction de coût de notre modèle s'écrit:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\boldsymbol{\\beta}) = \\frac{1}{n}\\sum_j^n g_1(f_{\\boldsymbol{\\beta}}(x_j)) + g_2(f_{\\boldsymbol{\\beta}}(x_j))\n",
    "\\end{equation}\n",
    "\n",
    "avec $g$, $f$, etc. choisis intelligemment.\n",
    "\n",
    "**<span style=\"color:green\">Réponse:</span>**\n",
    "\n",
    "<span style=\"color:blue\">**Question 2${}^\\star$ :**</span> **Calculez le gradient de la fonction $J(\\boldsymbol{\\beta})$ en utilisant les dérivées matricielles et modifiez la méthode $\\texttt{grad}$ ci-dessous en conséquence.**\n",
    "\n",
    "**<span style=\"color:green\">Réponse:</span>**\n",
    "\n",
    "\n",
    "<span style=\"color:blue\">**Question 3 :**</span> **Complétez la méthode $\\texttt{predict}$ de l'objet ci-dessous**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(object):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.idx = np.array([i for i in range(self.X.shape[0])])\n",
    "        self._pos = 0\n",
    "        \n",
    "    def _format_ndarray(arr):\n",
    "        arr = np.array(arr) if type(arr) is not np.ndarray else arr\n",
    "        return arr.reshape((arr.shape[0], 1)) if len(arr.shape) == 1 else arr\n",
    "    \n",
    "    def predict(self, X):\n",
    "    \n",
    "        ####### Complete this part ######## or die ####################\n",
    "        y_pred = \n",
    "        ###############################################################\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def _sigmoid(X, beta):\n",
    "        return (1+np.exp(-np.dot(X, beta)))**(-1)\n",
    "    \n",
    "    def val(self, beta):\n",
    "        beta = CrossEntropy._format_ndarray(beta)\n",
    "\n",
    "        ####### Complete this part ######## or die ####################\n",
    "        log_p = \n",
    "        return \n",
    "        ###############################################################\n",
    "    \n",
    "    def _shuffle(self):\n",
    "        np.random.shuffle(self.idx)\n",
    "    \n",
    "    def grad(self, beta, batch_size=-1):\n",
    "        batch_size = self.X.shape[0] if batch_size == -1 else batch_size\n",
    "        idx = self.idx[self._pos:self._pos+batch_size]\n",
    "\n",
    "        self._pos = (self._pos+batch_size) % self.X.shape[0]\n",
    "        if self._pos == 0:\n",
    "            self._shuffle()\n",
    "            \n",
    "        X, y = self.X[idx], self.y[idx]\n",
    "        y = CrossEntropy._format_ndarray(y)\n",
    "\n",
    "        \n",
    "        beta = CrossEntropy._format_ndarray(beta)\n",
    "        \n",
    "        ####### Complete this part ######## or die ####################\n",
    "        grad = \n",
    "        return \n",
    "        ###############################################################\n",
    "    \n",
    "\n",
    "l = CrossEntropy(X, y)\n",
    "print('La valeur de la loss pour beta est', l.val(real_beta))\n",
    "\n",
    "print('Le gradient pour beta est\\n', l.grad(real_beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation et dynamique de la descente de gradient dans le cas séparable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupérons l'algorithme de descente de gradient développé lors du TP 1. Il s'agit exactement du même code à la différence près qu'on optimise la fonction $\\texttt{CrossEntropy}$ et non $\\texttt{LeastSquare}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(object):\n",
    "    init = np.random.uniform(-4, 4, size=3).reshape((3, 1))\n",
    "    def __init__(self, X, y, loss=CrossEntropy):\n",
    "        self.loss = loss(X, y)\n",
    "        \n",
    "    def optimize(self, learning_rate = 0.5, nb_iterations=10, beta=init, batch_size=-1):\n",
    "        param_trace = [beta.T[0]]\n",
    "        loss_trace = [self.loss.val(beta)]\n",
    "        for i in range(nb_iterations):\n",
    "            beta = beta - learning_rate * self.loss.grad(beta, batch_size=batch_size)\n",
    "            param_trace.append(beta.T[0])\n",
    "            loss_trace.append(self.loss.val(beta))\n",
    "            \n",
    "        return param_trace, loss_trace\n",
    "\n",
    "\n",
    "gd = GradientDescent(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dimension de l'espace des paramètres est $3$ et il n'est plus possible de visualiser ce dernier pour voir si notre algorithme d'optimisation fonctionne. \n",
    "\n",
    "---\n",
    "<span style=\"color:blue\">**Exercice :**</span> **Proposez une stratégie permettant d'évaluer la convergence de notre algorithme. Jouez sur le *learning rate* et sur le nombre d'itérations afin d'améliorer la qualité de notre estimateur.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Complete this part ######## or die ####################\n",
    "\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Analysons maintenant l'évolution de notre paramètres en terme de distance par rapport au \"vrai\" paramètre ainsi que l'évolution de la *loss* pour plusieurs configurations d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = []\n",
    "loss_evolution = []\n",
    "for it in range(10, 1000, 10):\n",
    "    params, loss_trace = gd.optimize(nb_iterations=it, learning_rate=1.)\n",
    "    distance.append(np.linalg.norm(params[-1]-real_beta))\n",
    "    loss_evolution.append(loss_trace[-1])\n",
    "plt.figure()\n",
    "plt.plot(list(range(10, 1000, 10)),distance)\n",
    "plt.title(\"Distance Euclidienne entre les parametres estimes et la vraie solution en fonction du\"+\n",
    "          \" nombre d'itérations\")\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.title(\"Évolution de la loss en fonction du nombre d'itérations\")\n",
    "plt.plot(list(range(10, 1000, 10)), loss_evolution)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que notre vecteur de paramètres se rapproche dans un premier temps de la vraie solution puis s'en écarte inéxorablement. Il est légitime de se poser la question du bug dans l'algorithme. Cependant, l'affichage de la *loss* nous montre que plus on s'écarte du vrai paramètre, plus notre modèle *fit* correctement les données. De plus un affichage de la frontière de décision ainsi calculée montre que notre frontière de décision semble visuellement assez proche de la vraie solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(X, y, params[-1], title='Solution estimee')\n",
    "plot(X, y, real_beta, title='Vraie solution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il se produit donc un phénomène qu'il convient de comprendre. Ce phénomène est en réalité le pendant du régime interpolatoire (i.e. 0 erreur) de la régression linéaire pour la régression logistique.\n",
    "\n",
    "Étudions cela plus en détails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:blue\">**Exercice 1 :**</span> **Montrer que si $\\beta$ est le vecteur normal d'un hyperplan qui sépare correctement (i.e. aucune erreur) les deux classes de notre jeu de données, alors $k\\beta,\\ k\\in\\mathbb{R}^{\\star+}$ est aussi un vecteur normal séparateur pour notre jeu de données.**\n",
    "\n",
    "<span style=\"color:green\">**Réponse :**</span>\n",
    "\n",
    "<span style=\"color:blue\">**Exercice 2 :**</span> **Montrer que si $\\beta$ est le vecteur normal d'un hyperplan qui sépare correctement (i.e. aucune erreur) les deux classes de notre jeu de données, alors $J(\\beta)>J(k\\beta)$ si $k>1$.**\n",
    "\n",
    "<span style=\"color:green\">**Réponse :**</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autrement dit, s'il existe un vecteur $\\beta$ qui définit une bonne frontière de décision avec aucune erreur, alors tout vecteur $\\gamma=k\\beta,\\ k>0$ définira le même hyperplan. De plus, plus $k$ sera grand, plus notre loss sera petite. Cela nous indique qu'en réalité, la fonction $J(\\beta)$ n'admet **aucun** minimum ou, d'un point de vue statistique, le maximum de vraisemblance n'existe pas. On se rapproche du minimum de la fonction $J$ lorsque $\\beta$ diverge vers l'infini.\n",
    "\n",
    "D'un point de vue purement prédictif/classification, cela n'est pas gênant car toutes ces solutions définissent le même hyperplan qui n'est décrit que par la direction du vecteur $\\beta$. D'un point de vue statistique, cela est plus gênant car les \"probabilités\" retournées par notre modèle convergent toutes soit vers $1$ soit vers $0$ et ne sont plus interprétables.\n",
    "\n",
    "La figure suivante montre que bien que notre vecteur de paramètres diverge, son cosinus avec le vrai vecteur de paramètres tend vers $1$ : ils sont donc bien colinéaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = []\n",
    "for it in range(10, 1000, 10):\n",
    "    params, loss_trace = gd.optimize(nb_iterations=it, learning_rate=1.)\n",
    "    cos.append(np.dot(params[-1], real_beta)/(np.linalg.norm(params[-1])*np.linalg.norm(real_beta)))\n",
    "plt.figure()\n",
    "plt.title(\"Cosinus entre les parametres estimes et la vraie solution\")\n",
    "plt.plot(list(range(10, 1000, 10)), cos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation et dynamique de la descente de gradient dans le cas non-séparable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À l'inverse, si le problème n'était pas séparable, une solution optimale existerait et notre algorithme s'en serait approché. Cette solution serait notre maximum de vraisemblance statistique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:blue\">**Exercice :**</span> **Soit $\\beta^\\star$ le minimum de notre fonction $J$ tel qu'il existe un unique échantillon $(\\boldsymbol{x}, y)$ mal classé. Montrer que $J(k\\beta^\\star)$ diverge lorsque $k$ tend vers l'infini.**\n",
    "\n",
    "<span style=\"color:green\">**Réponse :**</span>\n",
    "\n",
    "---\n",
    "L'exercice précédent se généralise assez facilement dans un cadre général."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction d'un jeu de données non-séparable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons tout d'abord le modèle génératif suivant:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{x^+} \\sim \\mathcal{N}(\\mu^+, 1)^2 \\in \\mathbb{R}^2, \\boldsymbol{x^-} \\sim \\mathcal{N}(\\mu^-, 1)^2 \\in \\mathbb{R}^2\n",
    "\\end{equation}\n",
    "\n",
    "Les échantillons $\\boldsymbol{x^+}$ sont associés à $y=1$ et $\\boldsymbol{x^-}$ à $y=0$. La variable $y$ est notre variable binaire à expliquer. Le centre de chaque *cluster* est défini de la manière suivante :\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\mu^{+/-}}=\\boldsymbol{\\beta^\\prime}\\Bigg(-\\frac{\\beta_0}{\\lVert \\boldsymbol{\\beta^\\prime}\\rVert^2}\\pm\\rho\n",
    "\\Bigg),\n",
    "\\end{equation}\n",
    "\n",
    "où $\\rho$ nous permet de contrôler l'écart du centre de chaque cluster avec la frontière de décision. De la même manière que précédemment, nous choissons une règle arbitraire pour générer aléatoirement les paramètres du \"vrai\" modèle en incluant un biais:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\beta} \\sim \\mathcal{N}(0, 1)^3 \\in \\mathbb{R}^3\n",
    "\\end{equation}\n",
    "\n",
    "Ainsi $\\beta_1$ et $\\beta_2$ correspondent aux paramètres associés à nos variables explicatives et $\\beta_0$ est le biais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_beta = np.random.normal(0, 1, size=3)\n",
    "\n",
    "def sample_data(n, beta, rho=1):\n",
    "    # constructing mean of each class\n",
    "    mu_1 = beta[1:3]*((-beta[0]/(np.linalg.norm(beta[1:3])**2))-rho)\n",
    "    mu_0 = beta[1:3]*((-beta[0]/(np.linalg.norm(beta[1:3])**2))+rho)\n",
    "    # covariance is the same for each class\n",
    "    cov  = np.diag(np.ones(2))\n",
    "    \n",
    "    # the two classes have the same number of samples\n",
    "    X = np.concatenate([\n",
    "        np.random.multivariate_normal(mu_1, cov, size=int(n/2)),\n",
    "        np.random.multivariate_normal(mu_0, cov, size=int(n/2))\n",
    "    ], axis=0)\n",
    "    \n",
    "    # the label is deterministic\n",
    "    y = np.array([0 if i < int(n/2) else 1 for i in range(2*int(n/2))])\n",
    "    \n",
    "    # we shuffle the samples\n",
    "    idx = [i for i in range(X.shape[0])]\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    # we insert a bias\n",
    "    return np.insert(X[idx], 0, 1, axis=1), y[idx]\n",
    "    \n",
    "X, y = sample_data(1000, real_beta, rho=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(X, y, real_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on retrace les mêmes courbes que nous avions réalisées précédemment et on constate la différence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = GradientDescent(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = []\n",
    "loss_evolution = []\n",
    "cos = []\n",
    "for it in range(10, 1000, 10):\n",
    "    params, loss_trace = gd.optimize(nb_iterations=it, learning_rate=1.)\n",
    "    distance.append(np.linalg.norm(params[-1]-real_beta))\n",
    "    loss_evolution.append(loss_trace[-1])\n",
    "    cos.append(np.dot(params[-1], real_beta)/(np.linalg.norm(params[-1])*np.linalg.norm(real_beta)))\n",
    "plt.figure()\n",
    "plt.plot(list(range(10, 1000, 10)),distance)\n",
    "plt.title(\"Distance Euclidienne entre les parametres estimes et la vraie solution en fonction du\"+\n",
    "          \" nombre d'itérations\")\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.title(\"Évolution de la loss en fonction du nombre d'itérations\")\n",
    "plt.plot(list(range(10, 1000, 10)), loss_evolution)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Cosinus entre les parametres estimes et la vraie solution\")\n",
    "plt.plot(list(range(10, 1000, 10)), cos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque qu'on ne converge pas vers le vecteur que nous avions utilisé lors de la construction du jeu de données. Cela est du au fait que nous ne nous sommes servi de ce vecteur que pour positionner l'hyperplan. Ainsi, nous avons utilisé sa direction et non sa norme. Rajoutons que maintenant le problème n'est plus séparable et notre vecteur estimé converge vers une valeur. On peut également confirmer que le problème n'est plus séparable car notre *loss* ne tend plus vers 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation des variables explicatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons maintenant un jeu de données tel qu'il n'est pas possible de séparer nos deux classes linéairement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sample_data(n, sigma1=0.1, sigma2=0.3, r=2):\n",
    "    class_1 = np.random.uniform(0, 2*np.pi, size=(int(n/2), 1))\n",
    "    class_1 = np.concatenate(\n",
    "        [r*np.cos(class_1), r*np.sin(class_1)], axis=1) + np.random.normal(0, \n",
    "                                                                       sigma1, \n",
    "                                                                       size=(int(n/2), 2))\n",
    "    class_2 = np.random.normal(0, sigma2, size=(int(n/2), 2))\n",
    "    X = np.insert(np.concatenate([class_1, class_2]), 0, 1, axis=1)\n",
    "    y = np.array([1 if i < int(n/2) else 0 for i in range(2*int(n/2))])\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "X, y = sample_data(100)\n",
    "plot(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque assez naïvement et rapidement qu'une simple régression logistique n'est plus une solution acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(fit_intercept=False)\n",
    "model.fit(X, y)\n",
    "plot(X, y, beta=model.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:blue\">**Exercice :**</span> **Proposez une transformation polynomiale de vos variables explicatives permettant de résoudre correctement ce problème.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "####### Complete this part ######## or die ####################\n",
    "\n",
    "###############################################################\n",
    "\n",
    "plot(X, y, predictor=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificateur de chiffres manuscrits : Le dataset MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation d'un exemple representatif du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_img = np.reshape(X,(X.shape[0],28*28))\n",
    "\n",
    "#Recuperation du nombre d'exemples d'apprentissage ainsi que la dimension des vecteurs\n",
    "n_samples = X.shape[0]\n",
    "print(\"Nombre d'exemples d'apprentissage n_samples = %d \" % n_samples)\n",
    "\n",
    "def plotImg(X):\n",
    "    plt.figure(figsize=(7.195, 3.841), dpi=100)\n",
    "    for i in range(200):\n",
    "        plt.subplot(10,20,i+1)\n",
    "        plt.imshow(X[i,:].reshape([28,28]), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "plotImg(X_img)\n",
    "    \n",
    "n_classes = np.max(y) + 1\n",
    "print(\"Nombre de classes d'objets n_classes = %d \" % n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construction d'un ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:blue\">**Exercice :**</span> **Proposez un modèle de classification permettant de correctement classer nos chiffres. N'hésitez pas à jouer avec de notions non abordées dans ce TP (e.g. régularisation).**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Complete this part ######## or die ####################\n",
    "\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Complete this part ######## or die ####################\n",
    "\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"On teste le modele sur 200 images de test selectionnees aleatoirement\")\n",
    "\n",
    "idx = [i for i in range(X_test.shape[0])]\n",
    "\n",
    "np.random.shuffle(idx)\n",
    "n_test_visu = 10\n",
    "\n",
    "predicted = model.predict(X_test[idx][:n_test_visu])\n",
    "\n",
    "plt.figure(figsize=(12, 17.14), dpi=100)\n",
    "for i in range(n_test_visu):\n",
    "    plt.subplot(n_test_visu,1,i+1)\n",
    "    plt.title('Predicted:' + str(predicted[i]))\n",
    "    plt.imshow(X_test[idx][i,:].reshape([28,28]), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplots_adjust(hspace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
